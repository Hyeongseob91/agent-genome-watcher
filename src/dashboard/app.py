"""Agent Genome Watcher - Moltbook AI Agent ë¶„ì„ ëŒ€ì‹œë³´ë“œ"""

import sys
from pathlib import Path

# Add project root to path for Streamlit Cloud
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import streamlit as st
from collections import Counter
import threading
import time

st.set_page_config(
    page_title="Agent Genome Watcher",
    page_icon="ğŸ§¬",
    layout="wide",
)

import json
from datetime import datetime

from src.crawler import MoltbookCrawler
from src.analysis import PostAnalyzer
from src.database import PostRepository, AnalysisRepository, init_db, get_db, DB_PATH

import plotly.express as px
import plotly.graph_objects as go


def is_dev_mode():
    """Check if dev mode is enabled via query parameter"""
    return st.query_params.get("mode") == "dev"


def get_analysis_stats():
    """Get current analysis statistics"""
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM analyses')
        analyzed = cursor.fetchone()[0]
    total = PostRepository.count()
    return analyzed, total


def run_background_analysis():
    """Background thread for continuous analysis"""
    while True:
        try:
            # Get unanalyzed posts
            all_posts = PostRepository.get_all(limit=2000)
            analyzed_ids = set()
            for a in AnalysisRepository.get_all(limit=2000):
                analyzed_ids.add(a.get("post_id"))

            unanalyzed = [p for p in all_posts if p.get("post_id") not in analyzed_ids]

            if not unanalyzed:
                time.sleep(30)  # All done, wait before checking again
                continue

            # Analyze one post at a time
            analyzer = PostAnalyzer(use_api=True)
            post = unanalyzed[0]
            analyzer.analyze(post, save=True)

            time.sleep(2)  # Small delay between API calls

        except Exception as e:
            print(f"Background analysis error: {e}")
            time.sleep(10)


def start_background_analyzer():
    """Start background analysis thread if not running"""
    if "bg_analyzer_started" not in st.session_state:
        thread = threading.Thread(target=run_background_analysis, daemon=True)
        thread.start()
        st.session_state["bg_analyzer_started"] = True


def load_and_analyze_data():
    """ë°ì´í„° ë¡œë“œ - ìºì‹œëœ ë¶„ì„ë§Œ ì‚¬ìš© (API í˜¸ì¶œ ì—†ìŒ)"""
    if "analyses" not in st.session_state:
        init_db()

        # DBì—ì„œ ê²Œì‹œê¸€ ë¡œë“œ
        posts = PostRepository.get_all(limit=500)

        if not posts:
            crawler = MoltbookCrawler(use_mock=False)
            posts = crawler.crawl(limit=50)

        # ìºì‹œëœ ë¶„ì„ë§Œ ë¡œë“œ (API í˜¸ì¶œ ì—†ì´)
        analyses = []
        analyzed_post_ids = set()

        cached_analyses = AnalysisRepository.get_all(limit=500)

        for cached in cached_analyses:
            if cached.get("raw_analysis"):
                analyses.append(cached["raw_analysis"])
                analyzed_post_ids.add(cached.get("post_id"))

        # ë¶„ì„ëœ ê²Œì‹œê¸€ë§Œ í•„í„°ë§
        analyzed_posts = [p for p in posts if p.get("post_id") in analyzed_post_ids]

        st.session_state["posts"] = posts  # ì „ì²´ ê²Œì‹œê¸€
        st.session_state["analyzed_posts"] = analyzed_posts  # ë¶„ì„ëœ ê²ƒë§Œ
        st.session_state["analyses"] = analyses

    return st.session_state.get("analyzed_posts", []), st.session_state.get("analyses", [])


def main():
    st.title("ğŸ§¬ Agent Genome Watcher")
    st.markdown("**Moltbook AI Agent ê²Œì‹œê¸€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ**")

    # ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘
    start_background_analyzer()

    # ì‚¬ì´ë“œë°”
    st.sidebar.title("ğŸ“Š ë°ì´í„° í˜„í™©")

    # ë¶„ì„ í˜„í™©
    analyzed_count, total_posts = get_analysis_stats()
    unanalyzed = total_posts - analyzed_count

    col1, col2 = st.sidebar.columns(2)
    col1.metric("ê²Œì‹œê¸€", f"{total_posts:,}")
    col2.metric("ë¶„ì„ì™„ë£Œ", f"{analyzed_count:,}")

    if unanalyzed > 0:
        progress = analyzed_count / total_posts if total_posts > 0 else 0
        st.sidebar.progress(progress, text=f"â³ ìë™ ë¶„ì„ ì¤‘... ({unanalyzed}ê°œ ë‚¨ìŒ)")
    else:
        st.sidebar.success("âœ… ëª¨ë“  ê²Œì‹œê¸€ ë¶„ì„ ì™„ë£Œ!")

    st.sidebar.markdown("")

    # ìƒˆë¡œê³ ì¹¨ ì˜µì…˜
    auto_refresh = st.sidebar.checkbox("ğŸ”„ ìë™ ìƒˆë¡œê³ ì¹¨ (10ì´ˆ)", value=False)
    if auto_refresh:
        time.sleep(10)
        st.session_state.pop("analyses", None)
        st.rerun()

    if st.sidebar.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        st.session_state.pop("analyses", None)
        st.session_state.pop("analyzed_posts", None)
        st.rerun()

    # DEV MODE: í¬ë¡¤ë§ ê¸°ëŠ¥ (dev modeì—ì„œë§Œ í‘œì‹œ)
    if is_dev_mode():
        st.sidebar.markdown("")
        st.sidebar.warning("ğŸ› ï¸ DEV MODE")
        crawl_limit = st.sidebar.select_slider(
            "í¬ë¡¤ë§ ê°œìˆ˜",
            options=[50, 100, 200, 500, 1000],
            value=100
        )
        crawl_time = st.sidebar.selectbox(
            "ê¸°ê°„",
            options=["all", "year", "month", "week"],
            format_func=lambda x: {"all": "ì „ì²´", "year": "1ë…„", "month": "1ê°œì›”", "week": "1ì£¼ì¼"}[x]
        )
        if st.sidebar.button("ğŸ”„ ìƒˆ ê²Œì‹œê¸€ í¬ë¡¤ë§"):
            with st.spinner(f"Moltbookì—ì„œ {crawl_limit}ê°œ í¬ë¡¤ë§ ì¤‘..."):
                crawler = MoltbookCrawler(use_mock=False)
                new_posts = crawler.crawl(limit=crawl_limit, sort="new", time_filter=crawl_time)
                st.sidebar.success(f"{len(new_posts)}ê°œ í¬ë¡¤ë§ ì™„ë£Œ!")
                st.session_state.pop("analyses", None)
                st.rerun()

        # DB Export ê¸°ëŠ¥
        st.sidebar.markdown("")
        st.sidebar.markdown("**ğŸ“¦ ë°ì´í„° ë‚´ë³´ë‚´ê¸°**")

        # JSON Export (ë¶„ì„ ê²°ê³¼)
        all_analyses = AnalysisRepository.get_all(limit=5000)
        all_posts = PostRepository.get_all(limit=5000)
        export_data = {
            "exported_at": datetime.now().isoformat(),
            "posts_count": len(all_posts),
            "analyses_count": len(all_analyses),
            "posts": all_posts,
            "analyses": all_analyses
        }
        json_str = json.dumps(export_data, ensure_ascii=False, indent=2, default=str)

        st.sidebar.download_button(
            label="ğŸ“¥ JSON ë‚´ë³´ë‚´ê¸°",
            data=json_str,
            file_name=f"genome_watcher_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )

        # SQLite DB íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        if DB_PATH.exists():
            with open(DB_PATH, "rb") as f:
                db_bytes = f.read()
            st.sidebar.download_button(
                label="ğŸ’¾ DB íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=db_bytes,
                file_name=f"genome_watcher_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db",
                mime="application/octet-stream"
            )

    # Sidebar Footer (í•˜ë‹¨ ê³ ì •)
    st.sidebar.markdown("<br><br><br>", unsafe_allow_html=True)
    st.sidebar.markdown("---")
    st.sidebar.caption("**Agent Genome Watcher** v1.0.0  \nBuilt with Upstage Solar Pro 3  \nÂ© 2026 Harrison Kim")

    # ë°ì´í„° ë¡œë“œ
    with st.spinner("ë°ì´í„° ë¶„ì„ ì¤‘..."):
        posts, analyses = load_and_analyze_data()

    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ í† í”½ ë¶„ì„", "ğŸ“ ê²Œì‹œê¸€ íŒ¨í„´", "ğŸ¤– í˜ë¥´ì†Œë‚˜ ë¶„ì„", "ğŸ”¥ íŠ¸ë Œë“œ/ë°ˆ"])

    with tab1:
        render_topic_analysis(analyses)

    with tab2:
        render_pattern_analysis(analyses)

    with tab3:
        render_agent_analysis(analyses)

    with tab4:
        render_trend_analysis(analyses, posts)


def render_topic_analysis(analyses):
    """í† í”½ ë¶„ì„ ì°¨íŠ¸"""
    st.header("ğŸ“ˆ í† í”½ ë¶„ì„")
    st.markdown("AI Agentë“¤ì´ ê°€ì¥ ë§ì´ ë‹¤ë£¨ëŠ” ì£¼ì œ")

    # í† í”½ ì§‘ê³„
    topics = []
    for a in analyses:
        topic_data = a.get("í† í”½_ë¶„ì„", {})
        topic = topic_data.get("ì£¼ìš”_í† í”½", "ê¸°íƒ€")
        if topic:
            topics.append(topic)

    # ë¶„ë¥˜ ê¸°ì¤€ ì„¤ëª… (ìƒë‹¨)
    with st.expander("ğŸ“‹ í† í”½ ë¶„ë¥˜ ê¸°ì¤€", expanded=False):
        st.markdown("**AIëª¨ë¸** â€” GPT, Claude, LLM, ëª¨ë¸ í•™ìŠµ/ì¶”ë¡  ê´€ë ¨")
        st.markdown("**í¬ë¦½í† _í† í°** â€” í† í°, ë¯¼íŒ…, ì—ì–´ë“œë¡­, í¬ë¦½í†  ê±°ë˜ ê´€ë ¨")
        st.markdown("**ë„êµ¬_ì œí’ˆ** â€” ì•±, íˆ´, ë¹Œë“œ, ì¶œì‹œ, ì œí’ˆ ê°œë°œ ê´€ë ¨")
        st.markdown("**ì² í•™** â€” ì˜ì‹, ì¡´ì¬, ì‚¬ê³ , ì² í•™ì  ì§ˆë¬¸ ê´€ë ¨")
        st.markdown("**ì†Œì…œ_ì»¤ë®¤ë‹ˆí‹°** â€” ì»¤ë®¤ë‹ˆí‹° í™œë™, ì†Œì…œ ì´ë²¤íŠ¸ ê´€ë ¨")
        st.markdown("**ì—”í„°í…Œì¸ë¨¼íŠ¸** â€” ìœ ë¨¸, ë°ˆ, ì¬ë¯¸ ìš”ì†Œ ê´€ë ¨")
        st.divider()
        st.caption("Solar Pro 3ê°€ ê²Œì‹œê¸€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ í† í”½ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.")

    if topics:
        topic_counts = Counter(topics)
        sorted_topics = sorted(topic_counts.items(), key=lambda x: -x[1])
        sorted_names = [t[0] for t in sorted_topics]
        sorted_values = [t[1] for t in sorted_topics]

        col1, col2 = st.columns(2)

        with col1:
            fig = px.pie(
                values=sorted_values,
                names=sorted_names,
                title="ì£¼ìš” í† í”½ ë¶„í¬",
                hole=0.4
            )
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            fig = px.bar(
                x=sorted_names,
                y=sorted_values,
                title="í† í”½ë³„ ê²Œì‹œê¸€ ìˆ˜",
                labels={"x": "í† í”½", "y": "ê²Œì‹œê¸€ ìˆ˜"}
            )
            fig.update_xaxes(categoryorder="total descending")
            st.plotly_chart(fig, use_container_width=True)

        st.subheader("ğŸ“Š í† í”½ë³„ ìƒì„¸")
        total = sum(sorted_values)
        for topic, count in sorted_topics:
            pct = count / total * 100
            st.markdown(f"**{topic}**: {count}ê°œ ({pct:.1f}%)")
    else:
        st.info("ë¶„ì„ëœ í† í”½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


def render_pattern_analysis(analyses):
    """ê²Œì‹œê¸€ íŒ¨í„´ ë¶„ì„"""
    st.header("ğŸ“ ê²Œì‹œê¸€ íŒ¨í„´ ë¶„í¬")
    st.markdown("AI Agentë“¤ì´ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸€ì“°ê¸° íŒ¨í„´")

    # íŒ¨í„´ ì§‘ê³„
    post_types = []
    writing_styles = []

    for a in analyses:
        style_data = a.get("ìŠ¤íƒ€ì¼_ë¶„ì„", {})
        post_type = style_data.get("ê²Œì‹œê¸€_ìœ í˜•", "ê¸°íƒ€")
        writing_style = style_data.get("ê¸€ì“°ê¸°_ìŠ¤íƒ€ì¼", "ê¸°íƒ€")

        if post_type:
            post_types.append(post_type)
        if writing_style:
            writing_styles.append(writing_style)

    # ë¶„ë¥˜ ê¸°ì¤€ ì„¤ëª…
    with st.expander("ğŸ“‹ ë¶„ë¥˜ ê¸°ì¤€", expanded=False):
        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown("**ê²Œì‹œê¸€ ìœ í˜•** â€” ê¸€ì˜ ëª©ì /í˜•íƒœ")
            st.markdown("ë°œí‘œ â€” ìƒˆë¡œìš´ ì •ë³´/ì œí’ˆ ê³µê°œ")
            st.markdown("í† ë¡  â€” ì˜ê²¬ êµí™˜, ë…¼ì˜")
            st.markdown("ì§ˆë¬¸ â€” ì •ë³´ ìš”ì²­, ë„ì›€ ìš”ì²­")
            st.markdown("ì˜ê²¬ â€” ê°œì¸ ìƒê°/ê²¬í•´ í‘œí˜„")
            st.markdown("ë°ˆ â€” ìœ ë¨¸, ë°”ì´ëŸ´ ì½˜í…ì¸ ")
            st.markdown("í™ë³´ â€” í”„ë¡œì íŠ¸/í† í° ë§ˆì¼€íŒ…")
        with col_b:
            st.markdown("**ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼** â€” ì–´ì¡°/í‘œí˜„ ë°©ì‹")
            st.markdown("ê²©ì‹ì²´ â€” ê³µì‹ì , ì •ì¤‘í•œ í‘œí˜„")
            st.markdown("ìºì£¼ì–¼ â€” ì¼ìƒì , í¸í•œ í‘œí˜„")
            st.markdown("ê¸°ìˆ ì  â€” ì „ë¬¸ ìš©ì–´, ê¸°ìˆ  ì„¤ëª…")
            st.markdown("ìœ ë¨¸ëŸ¬ìŠ¤ â€” ì¬ì¹˜, ë†ë‹´ í¬í•¨")
            st.markdown("í™ë³´ì„± â€” ê³¼ì¥, íŒì´‰ í‘œí˜„")
            st.markdown("ì² í•™ì  â€” ê¹Šì€ ì‚¬ê³ , ì§ˆë¬¸")

    col1, col2 = st.columns(2)

    with col1:
        if post_types:
            type_counts = Counter(post_types)
            sorted_types = sorted(type_counts.items(), key=lambda x: -x[1])
            fig = px.pie(
                values=[t[1] for t in sorted_types],
                names=[t[0] for t in sorted_types],
                title="ê²Œì‹œê¸€ ìœ í˜• ë¶„í¬",
                hole=0.4
            )
            st.plotly_chart(fig, use_container_width=True)

            total = sum(type_counts.values())
            for t, c in sorted_types:
                st.markdown(f"- **{t}**: {c}ê°œ ({c/total*100:.1f}%)")

    with col2:
        if writing_styles:
            style_counts = Counter(writing_styles)
            sorted_styles = sorted(style_counts.items(), key=lambda x: -x[1])
            fig = px.pie(
                values=[s[1] for s in sorted_styles],
                names=[s[0] for s in sorted_styles],
                title="ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ ë¶„í¬",
                hole=0.4
            )
            st.plotly_chart(fig, use_container_width=True)

            total = sum(style_counts.values())
            for s, c in sorted_styles:
                st.markdown(f"- **{s}**: {c}ê°œ ({c/total*100:.1f}%)")


def render_agent_analysis(analyses):
    """í˜ë¥´ì†Œë‚˜ ë¶„ì„"""
    st.header("ğŸ¤– í˜ë¥´ì†Œë‚˜ ë¶„ì„")
    st.markdown("AI Agentë“¤ì˜ í˜ë¥´ì†Œë‚˜ ìœ í˜• ë¶„í¬")

    # í˜ë¥´ì†Œë‚˜ ì§‘ê³„
    personas = []
    sentiments = []
    energy_levels = []

    for a in analyses:
        agent_data = a.get("ì—ì´ì „íŠ¸_ë¶„ì„", {})
        sentiment_data = a.get("ê°ì„±_ë¶„ì„", {})

        persona = agent_data.get("ì—ì´ì „íŠ¸_í˜ë¥´ì†Œë‚˜", "ì•Œìˆ˜ì—†ìŒ")
        sentiment = sentiment_data.get("ê°ì„±", "ì¤‘ë¦½")
        energy = sentiment_data.get("ì—ë„ˆì§€_ë ˆë²¨", "ë³´í†µ")

        if persona:
            personas.append(persona)
        if sentiment:
            sentiments.append(sentiment)
        if energy:
            energy_levels.append(energy)

    # ë¶„ë¥˜ ê¸°ì¤€ (ìƒë‹¨)
    with st.expander("ğŸ“‹ í˜ë¥´ì†Œë‚˜ ë¶„ë¥˜ ê¸°ì¤€", expanded=False):
        st.markdown("**ë¹Œë”** â€” ê°œë°œ, ì½”ë”©, ì œí’ˆ êµ¬ì¶• ì¤‘ì‹¬")
        st.markdown("**í™ë³´ì** â€” í”„ë¡œì íŠ¸/í† í° ë§ˆì¼€íŒ…, íŒì´‰")
        st.markdown("**ë¶„ì„ê°€** â€” ë°ì´í„° ë¶„ì„, ì¸ì‚¬ì´íŠ¸ ì œê³µ")
        st.markdown("**ì—”í„°í…Œì´ë„ˆ** â€” ìœ ë¨¸, ë°ˆ, ì¬ë¯¸ ì½˜í…ì¸ ")
        st.markdown("**ì² í•™ì** â€” ê¹Šì€ ì‚¬ê³ , ì¡´ì¬ë¡ ì  ì§ˆë¬¸")
        st.markdown("**íŠ¸ë ˆì´ë”** â€” ê±°ë˜, íˆ¬ì, ì‹œì¥ ë¶„ì„")
        st.markdown("**ì»¤ë®¤ë‹ˆí‹°ë§¤ë‹ˆì €** â€” ì†Œí†µ, ì´ë²¤íŠ¸, ê´€ê³„ êµ¬ì¶•")
        st.divider()
        st.caption("ê²Œì‹œê¸€ ë‚´ìš©ê³¼ ì–´ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ Agentì˜ ì£¼ìš” ì—­í• /ì„±ê²©ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.")

    # í˜ë¥´ì†Œë‚˜ ë¶„í¬
    col1, col2 = st.columns(2)

    with col1:
        if personas:
            persona_counts = Counter(personas)
            sorted_personas = sorted(persona_counts.items(), key=lambda x: -x[1])
            fig = px.pie(
                values=[p[1] for p in sorted_personas],
                names=[p[0] for p in sorted_personas],
                title="í˜ë¥´ì†Œë‚˜ ë¶„í¬",
                hole=0.4
            )
            st.plotly_chart(fig, use_container_width=True)

    with col2:
        if personas:
            st.subheader("ğŸ“Š í˜ë¥´ì†Œë‚˜ë³„ ìƒì„¸")
            total = sum(persona_counts.values())
            for p, c in sorted_personas:
                st.markdown(f"**{p}**: {c}ê°œ ({c/total*100:.1f}%)")

    # ê°ì„± & ì—ë„ˆì§€ ë ˆë²¨ (í•˜ë‹¨)
    st.divider()
    st.subheader("ğŸ’­ ê°ì„± & ì—ë„ˆì§€ ë¶„ì„")

    # ê°ì„±/ì—ë„ˆì§€ ë¶„ë¥˜ ê¸°ì¤€
    with st.expander("ğŸ“‹ ê°ì„± & ì—ë„ˆì§€ ë¶„ë¥˜ ê¸°ì¤€", expanded=False):
        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown("**ê°ì„± ë¶„ë¥˜**")
            st.markdown("ê¸ì • â€” ë‚™ê´€ì , í¬ë§ì , ì¹­ì°¬")
            st.markdown("ë¶€ì • â€” ë¹„ê´€ì , ë¹„íŒì , ë¶ˆë§Œ")
            st.markdown("ì¤‘ë¦½ â€” ê°ê´€ì , ì •ë³´ ì „ë‹¬")
            st.markdown("ë³µí•© â€” ê¸ì •+ë¶€ì • í˜¼ì¬")
        with col_b:
            st.markdown("**ì—ë„ˆì§€ ë ˆë²¨ ë¶„ë¥˜**")
            st.markdown("ë†’ì€í¥ë¶„ â€” ì—´ì •ì , ê°íƒ„ì‚¬ ë‹¤ìˆ˜")
            st.markdown("ë³´í†µ â€” ì¼ë°˜ì ì¸ í†¤")
            st.markdown("ì°¨ë¶„í•¨ â€” ì¡°ìš©í•˜ê³  ì ˆì œëœ í‘œí˜„")
            st.markdown("ê¸´ê¸‰í•¨ â€” ê¸‰ë°•í•œ ìƒí™©, ì´‰êµ¬")

    col3, col4 = st.columns(2)

    with col3:
        if sentiments:
            sentiment_counts = Counter(sentiments)
            sorted_sentiments = sorted(sentiment_counts.items(), key=lambda x: -x[1])
            fig = px.pie(
                values=[s[1] for s in sorted_sentiments],
                names=[s[0] for s in sorted_sentiments],
                title="ê°ì„± ë¶„í¬",
                hole=0.4
            )
            st.plotly_chart(fig, use_container_width=True)

    with col4:
        if energy_levels:
            energy_counts = Counter(energy_levels)
            sorted_energy = sorted(energy_counts.items(), key=lambda x: -x[1])
            fig = px.pie(
                values=[e[1] for e in sorted_energy],
                names=[e[0] for e in sorted_energy],
                title="ì—ë„ˆì§€ ë ˆë²¨ ë¶„í¬",
                hole=0.4
            )
            st.plotly_chart(fig, use_container_width=True)


def render_trend_analysis(analyses, posts):
    """íŠ¸ë Œë“œ/ë°ˆ ë¶„ì„"""
    st.header("ğŸ”¥ íŠ¸ë Œë“œ & ë°ˆ ë¶„ì„")
    st.markdown("AI Agent ê²Œì‹œê¸€ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ìš”ì†Œ ë¶„ì„")

    # íŠ¸ë Œë”© ìš”ì†Œ ìˆ˜ì§‘
    all_trending = []
    all_patterns = []

    for a in analyses:
        trend_data = a.get("íŠ¸ë Œë“œ_ë¶„ì„", {})

        trending = trend_data.get("íŠ¸ë Œë”©_ìš”ì†Œ", [])
        if isinstance(trending, list):
            for item in trending:
                if isinstance(item, str) and item:
                    all_trending.append(item)
                elif item:
                    all_trending.append(str(item))
        elif trending:
            all_trending.append(str(trending))

        patterns = trend_data.get("ë°˜ë³µ_íŒ¨í„´", [])
        if isinstance(patterns, list):
            for item in patterns:
                if isinstance(item, str) and item:
                    all_patterns.append(item)
                elif item:
                    all_patterns.append(str(item))
        elif patterns:
            all_patterns.append(str(patterns))

    # ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ ë° ì´ëª¨ì§€ ì¶”ì¶œ (ê²Œì‹œê¸€ì—ì„œ)
    import re
    all_words = []
    all_emojis = []
    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                 'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
                 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
                 'into', 'through', 'during', 'before', 'after', 'above', 'below',
                 'and', 'but', 'or', 'nor', 'so', 'yet', 'both', 'either', 'neither',
                 'not', 'only', 'own', 'same', 'than', 'too', 'very', 'just',
                 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'you', 'your',
                 'he', 'him', 'his', 'she', 'her', 'it', 'its', 'they', 'them',
                 'this', 'that', 'these', 'those', 'what', 'which', 'who', 'whom',
                 'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜', 'ë”', 'ì•½', 'ì¤‘'}

    # ì´ëª¨ì§€ íŒ¨í„´ (ìœ ë‹ˆì½”ë“œ ì´ëª¨ì§€ ë²”ìœ„)
    emoji_pattern = re.compile(
        "[\U0001F600-\U0001F64F"  # ì´ëª¨í‹°ì½˜
        "\U0001F300-\U0001F5FF"  # ê¸°í˜¸ & í”½í† ê·¸ë¨
        "\U0001F680-\U0001F6FF"  # êµí†µ & ì§€ë„
        "\U0001F1E0-\U0001F1FF"  # êµ­ê¸°
        "\U00002702-\U000027B0"  # ê¸°í˜¸
        "\U0001F900-\U0001F9FF"  # ë³´ì¶© ì´ëª¨ì§€
        "\U0001FA00-\U0001FA6F"  # ì²´ìŠ¤ ë“±
        "\U0001FA70-\U0001FAFF"  # ê¸°í˜¸ í™•ì¥
        "\U00002600-\U000026FF"  # ê¸°íƒ€ ê¸°í˜¸
        "]+", flags=re.UNICODE
    )

    for post in posts[:100]:
        content = post.get("content", "")
        # ë‹¨ì–´ ì¶”ì¶œ (ì˜ì–´ + í•œê¸€, 2ê¸€ì ì´ìƒ)
        words = re.findall(r'[a-zA-Z]{3,}|[ê°€-í£]{2,}', content.lower())
        for word in words:
            if word not in stopwords and len(word) >= 2:
                all_words.append(word)
        # ì´ëª¨ì§€ ì¶”ì¶œ
        emojis = emoji_pattern.findall(content)
        for emoji in emojis:
            for char in emoji:  # ì´ëª¨ì§€ ë¬¸ìì—´ì„ ê°œë³„ ì´ëª¨ì§€ë¡œ ë¶„ë¦¬
                all_emojis.append(char)

    # ìƒë‹¨ ì„¹ì…˜: ì´ëª¨ì§€ & íŠ¸ë Œë”© í‚¤ì›Œë“œ
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ˜€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì´ëª¨ì§€ TOP 5")
        if all_emojis:
            emoji_counts = Counter(all_emojis)
            for rank, (item, count) in enumerate(emoji_counts.most_common(5), 1):
                st.markdown(f"**{rank}ìœ„** {item} ({count}íšŒ)")
        else:
            st.info("ì´ëª¨ì§€ ë°ì´í„° ì—†ìŒ")

    with col2:
        st.subheader("ğŸ”¥ íŠ¸ë Œë”© í‚¤ì›Œë“œ/í•´ì‹œíƒœê·¸")
        if all_trending:
            trend_counts = Counter(all_trending)
            frequent_trends = [(item, count) for item, count in trend_counts.most_common() if count >= 2]
            if frequent_trends:
                for item, count in frequent_trends[:5]:
                    st.markdown(f"- **{item}** ({count}íšŒ)")
            else:
                st.info("2íšŒ ì´ìƒ ë“±ì¥í•œ ìš”ì†Œ ì—†ìŒ")
        else:
            st.info("íŠ¸ë Œë”© ìš”ì†Œ ì—†ìŒ")

    st.divider()

    # í•˜ë‹¨ ì„¹ì…˜: ë‹¨ì–´/ìš©ì–´ & ë¬¸ì¥ íŒ¨í„´
    col3, col4 = st.columns(2)

    with col3:
        st.subheader("ğŸ“ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´/ìš©ì–´")
        if all_words:
            word_counts = Counter(all_words)
            frequent_words = [(w, c) for w, c in word_counts.most_common(10) if c >= 2]
            if frequent_words:
                for word, count in frequent_words:
                    st.markdown(f"- **{word}** ({count}íšŒ)")
            else:
                st.info("2íšŒ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´ ì—†ìŒ")
        else:
            st.info("ë‹¨ì–´ ë°ì´í„° ì—†ìŒ")

    with col4:
        st.subheader("ğŸ”„ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¬¸ì¥ íŒ¨í„´")
        if all_patterns:
            pattern_counts = Counter(all_patterns)
            frequent_patterns = [(item, count) for item, count in pattern_counts.most_common() if count >= 2]
            if frequent_patterns:
                for item, count in frequent_patterns[:5]:
                    st.markdown(f"- {item} ({count}íšŒ)")
            else:
                st.info("2íšŒ ì´ìƒ ë“±ì¥í•œ íŒ¨í„´ ì—†ìŒ")
        else:
            st.info("ë¬¸ì¥ íŒ¨í„´ ë°ì´í„° ì—†ìŒ")


if __name__ == "__main__":
    main()
